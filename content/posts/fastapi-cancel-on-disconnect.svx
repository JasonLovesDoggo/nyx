---
title: Stop Wasting CPU: Kill Dead FastAPI Streams When Clients Bail
description: Monitor FastAPI client connections and auto-cancel abandoned streaming tasks with structured concurrency
published_at: '2025-01-16'
categories:
  - fastapi
  - python
  - async
  - streaming
---

## The Problem is Nasty

When a client disconnects from your FastAPI streaming endpoint, your server doesn't care. It keeps churning through CPU, maybe holding DB connections open, processing ML models — all for nobody. 

Multiply that by 100 dead connections during a traffic spike, and you've got a resource graveyard. Your server is burning cycles on ghost requests while real users get slower responses.

## The Solution: cancel_on_disconnect

Here's a robust context manager that monitors HTTP connections and cancels tasks when clients disconnect:

```python
import logging
from contextlib import asynccontextmanager
from anyio import create_task_group
from fastapi import Request

logger = logging.getLogger(__name__)

@asynccontextmanager
async def cancel_on_disconnect(request: Request):
    """
    Async context manager for async code that needs to be cancelled 
    if client disconnects prematurely.
    """
    async with create_task_group() as tg:
        
        async def watch_disconnect():
            while True:
                message = await request.receive()
                
                if message["type"] == "http.disconnect":
                    client = f"{request.client.host}:{request.client.port}" if request.client else "-:-"
                    logger.debug(f'{client} - "{request.method} {request.url.path}" 499 DISCONNECTED')
                    
                    tg.cancel_scope.cancel()
                    break
        
        tg.start_soon(watch_disconnect)
        
        try:
            yield
        finally:
            tg.cancel_scope.cancel()
```

## How It Works (And Why You Care)

1. **Task Group = No Stray Tasks**: You don't want abandoned tasks leaking across your app. The task group ensures when one task dies (disconnect), the whole operation shuts down clean.

2. **Background Watcher**: Spawns a task to monitor `http.disconnect` messages from the ASGI layer. This catches browser tab closes, network drops, impatient users.

3. **Instant Kill Switch**: When disconnect hits, it cancels the entire task group. Your expensive ML inference stops immediately instead of running for another 30 seconds.

4. **Guaranteed Cleanup**: Whether your task finishes normally or gets axed, cleanup always runs.

⚠️ **Heads up**: `request.receive()` is specific to FastAPI's ASGI layer and not documented public API. Works great with uvicorn + FastAPI, but test in your actual deployment stack since proxies and load balancers handle disconnects differently.

## Real Usage Example

```python
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.post("/analyze-video")
async def analyze_video(request: Request):
    async def generate():
        try:
            async with cancel_on_disconnect(request):
                # Process 1000 video frames - expensive!
                for frame_idx in range(1000):
                    # This could take 500ms per frame
                    detections = await run_yolo_inference(frame_idx)
                    yield f"data: Frame {frame_idx}: found {len(detections)} objects\n"
        except Exception as e:
            logger.info(f"Video analysis cancelled: {e}")
            
    return StreamingResponse(generate(), media_type="text/plain")
```

## What Breaks Without This

**Memory Leaks**: Dead connections keep tasks alive, holding references to large objects. Your RAM usage creeps up until OOM kill.

**DB Lock Hell**: Half-finished transactions hold locks. Other requests timeout waiting for resources that'll never be released.

**Silent Resource Waste**: Your beautiful monitoring dashboard shows "everything fine" while 40% of your CPU burns on work nobody wants.

**Cascading Failures**: Slow responses from wasted cycles make healthy requests timeout, triggering circuit breakers.

## The Fix In Action

```python
async with cancel_on_disconnect(request):
    # This expensive work stops immediately when client bails
    async for batch in process_million_rows():
        yield f"data: Processed batch {batch.id}\n"
```

Without this pattern, that million-row job keeps running even after the user closed their browser. With it, disconnection kills the task instantly.

## Impact: Before vs After

In a real service processing ML workloads, this pattern reduced:
- Orphaned streaming tasks from ~40/sec to zero during peak
- Average CPU utilization by 15% 
- P95 response times by 200ms (less resource contention)
- Out-of-memory incidents from daily to never

## Integration with Database Operations

Shield critical database operations from cancellation:

```python
async with cancel_on_disconnect(request):
    # This can be cancelled
    async for chunk in ai_processing():
        yield chunk
        
# Shield database saves from cancellation
async def save_results():
    async with database.transaction():
        await save_data()
        
await asyncio.shield(save_results())
```

## Things You'll Get Wrong First Time

1. **Forgetting to shield DB commits**: Your transaction gets cancelled mid-commit, leaving corrupted state. Always wrap critical DB operations in `asyncio.shield()`.

2. **Not testing with real proxies**: Works perfect locally, breaks in production behind nginx because proxy buffering changes disconnect behavior.

3. **Ignoring partial state**: User cancels after processing 90% of data. Save progress or they'll be pissed restarting from zero.

4. **Missing cleanup on cancellation**: File handles, temp data, external service connections — they all leak if you don't handle the cancel path.

## Test Your Disconnects

```bash
# Start your stream, then kill the connection
curl -X POST http://localhost:8000/analyze-video &
PID=$!
sleep 2  # Let it start processing 
kill $PID  # Simulate disconnect
```

Check your logs for the disconnect message and verify your expensive work actually stops.

This pattern isn't academic — it's the difference between a server that scales and one that melts under load.